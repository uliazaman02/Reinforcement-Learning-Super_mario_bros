{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn-Ld2fvj25E"
      },
      "outputs": [],
      "source": [
        "!pip install nes-py==0.2.6\n",
        "!pip install gym-super-mario-bros\n",
        "!apt-get update\n",
        "!apt-get install ffmpeg libsm6 libxext6  -y\n",
        "!apt install -y libgl1-mesa-glx\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "from gym import Wrapper\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym.spaces import Box\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import numpy as np\n",
        "import cv2\n",
        "import collections\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "R_qOudUNkNdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = True"
      ],
      "metadata": {
        "id": "tGzxpb9Nx26O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.lstm = nn.LSTMCell(32 * 6 * 6, 512)\n",
        "        self.critic_linear = nn.Linear(512, 1)\n",
        "        self.actor_linear = nn.Linear(512, num_actions)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                # nn.init.kaiming_uniform_(module.weight)\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.LSTMCell):\n",
        "                nn.init.constant_(module.bias_ih, 0)\n",
        "                nn.init.constant_(module.bias_hh, 0)\n",
        "\n",
        "    def forward(self, x, hx, cx):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        hx, cx = self.lstm(x.view(x.size(0), -1), (hx, cx))\n",
        "        return self.actor_linear(hx), self.critic_linear(hx), hx, cx"
      ],
      "metadata": {
        "id": "a_KlUIusA8AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_frame(frame):\n",
        "    if frame is not None:\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (84, 84))[None, :, :] / 255.\n",
        "        return frame\n",
        "    else:\n",
        "        return np.zeros((1, 84, 84))\n",
        "\n",
        "class CustomReward(Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(CustomReward, self).__init__(env)\n",
        "        self.observation_space = Box(low=0, high=255, shape=(1, 84, 84))\n",
        "        self.curr_score = 0\n",
        "        self.prev_x = None\n",
        "\n",
        "    def step(self, action):\n",
        "        # print(action)\n",
        "        state, reward, done, info = self.env.step(action)\n",
        "        state = process_frame(state)\n",
        "        reward += (info[\"score\"] - self.curr_score) / 100.\n",
        "        if self.prev_x != None:\n",
        "          if(info[\"x_pos\"] - self.prev_x <= 0):\n",
        "            reward += -1\n",
        "        self.prev_x = info[\"x_pos\"]\n",
        "        self.curr_score = info[\"score\"]\n",
        "\n",
        "        if done:\n",
        "            if info[\"flag_get\"]:\n",
        "                reward += 50\n",
        "            else:\n",
        "                reward -= 50\n",
        "        return state, reward / 10., done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.curr_score = 0\n",
        "        return process_frame(self.env.reset())\n",
        "\n",
        "\n",
        "class CustomSkipFrame(Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        super(CustomSkipFrame, self).__init__(env)\n",
        "        self.observation_space = Box(low=0, high=255, shape=(4, 84, 84))\n",
        "        self.skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0\n",
        "        states = []\n",
        "        state, reward, done, info = self.env.step(action)\n",
        "        for i in range(self.skip):\n",
        "            if not done:\n",
        "                state, reward, done, info = self.env.step(action)\n",
        "                total_reward += reward\n",
        "                states.append(state)\n",
        "            else:\n",
        "                states.append(state)\n",
        "        states = np.concatenate(states, 0)[None, :, :, :]\n",
        "        return states.astype(np.float32), reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        state = self.env.reset()\n",
        "        states = np.concatenate([state for _ in range(self.skip)], 0)[None, :, :, :]\n",
        "        return states.astype(np.float32)"
      ],
      "metadata": {
        "id": "rMOzOY_ZBFRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalAdam(torch.optim.Adam):\n",
        "    def __init__(self, params, lr):\n",
        "        super(GlobalAdam, self).__init__(params, lr=lr)\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                # state['step'] = 0\n",
        "                state['step'] = torch.zeros(1)\n",
        "                state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                state['exp_avg'].share_memory_()\n",
        "                state['exp_avg_sq'].share_memory_()"
      ],
      "metadata": {
        "id": "Dz3B4tg_RtJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_local_steps = 300\n",
        "episode_max = 500\n",
        "\n",
        "gamma = 0.9\n",
        "tau = 1.0\n",
        "beta = 0.1 # change from 0.01\n",
        "\n",
        "world = 1\n",
        "stage = 1\n",
        "\n",
        "max_actions = 200\n",
        "num_global_steps = 5e6\n",
        "\n",
        "action_type = SIMPLE_MOVEMENT"
      ],
      "metadata": {
        "id": "K8ys4jYG9RiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  torch.manual_seed(123)\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "  env = gym_super_mario_bros.make(\"SuperMarioBros-{}-{}-v0\".format(world, stage))\n",
        "  env = JoypadSpace(env, action_type)\n",
        "  env = CustomReward(env)\n",
        "  env = CustomSkipFrame(env)\n",
        "\n",
        "  local_model = ActorCritic(env.observation_space.shape[0], len(action_type))\n",
        "\n",
        "  state = torch.from_numpy(env.reset()).to(device)\n",
        "\n",
        "  done = True\n",
        "  curr_step = 0\n",
        "  curr_episode = 0\n",
        "\n",
        "  local_model.load_state_dict(torch.load(\"./a2c_super_mario_bros_1_1_S.pt\"))\n",
        "\n",
        "  local_model.to(device)\n",
        "  local_model.train()\n",
        "\n",
        "  #########\n",
        "  optimizer = GlobalAdam(local_model.parameters(), lr=1e-4)\n",
        "  #########\n",
        "\n",
        "  while curr_episode <= episode_max:\n",
        "    curr_episode += 1\n",
        "\n",
        "    if curr_episode % 100 == 0:\n",
        "      torch.save(local_model.state_dict(), \"./a2c_super_mario_bros_1_1_S.pt\")\n",
        "      print(\"Episode {}\".format(curr_episode))\n",
        "\n",
        "      for name, param in local_model.named_parameters():\n",
        "        print(name, param)\n",
        "        print()\n",
        "\n",
        "    if done:\n",
        "      h_0 = torch.zeros((1, 512), dtype=torch.float).to(device)\n",
        "      c_0 = torch.zeros((1, 512), dtype=torch.float).to(device)\n",
        "    else:\n",
        "      h_0 = h_0.detach().to(device)\n",
        "      c_0 = c_0.detach().to(device)\n",
        "\n",
        "    log_policies = []\n",
        "    values = []\n",
        "    rewards = []\n",
        "    entropies = []\n",
        "\n",
        "    for _ in range(num_local_steps):\n",
        "      curr_step += 1\n",
        "      logits, value, h_0, c_0 = local_model.forward(state, h_0, c_0)\n",
        "      policy = F.softmax(logits, dim=1)\n",
        "      log_policy = F.softmax(logits, dim=1)\n",
        "      entropy = -(policy * log_policy).sum(1, keepdim=True)\n",
        "\n",
        "      m = Categorical(policy)\n",
        "      action = m.sample().item()\n",
        "\n",
        "      state, reward, done, info = env.step(action)\n",
        "      state = torch.from_numpy(state).to(device)\n",
        "\n",
        "      values.append(value)\n",
        "      log_policies.append(log_policy[0, action])\n",
        "      rewards.append(reward)\n",
        "      entropies.append(entropy)\n",
        "\n",
        "      if done:\n",
        "        curr_step = 0\n",
        "        state = torch.from_numpy(env.reset()).to(device)\n",
        "        break\n",
        "\n",
        "    print(info[\"score\"], info[\"flag_get\"])\n",
        "    R = torch.zeros((1, 1), dtype=torch.float).to(device)\n",
        "\n",
        "    if not done:\n",
        "      _, R, _, _ = local_model.forward(state, h_0, c_0) # bootstrap from last state\n",
        "\n",
        "    actor_loss = 0\n",
        "    critic_loss = 0\n",
        "    entropy_loss = 0\n",
        "    gae = torch.zeros((1, 1), dtype=torch.float).to(device)\n",
        "    next_value = R\n",
        "\n",
        "    for value, log_policy, reward, entropy in list(zip(values, log_policies, rewards, entropies))[::-1]:\n",
        "            gae = gae * gamma * tau\n",
        "            gae = gae + reward + gamma * next_value.detach() - value.detach()\n",
        "            next_value = value\n",
        "            actor_loss = actor_loss + log_policy * gae\n",
        "            R = R * gamma + reward\n",
        "            critic_loss = critic_loss + (R - value) ** 2 / 2\n",
        "            entropy_loss = entropy_loss + entropy\n",
        "\n",
        "\n",
        "    total_loss = -actor_loss + critic_loss - beta * entropy_loss\n",
        "    #####\n",
        "    optimizer.zero_grad()\n",
        "    #####\n",
        "    total_loss.backward()\n",
        "\n",
        "    ####\n",
        "    optimizer.step()\n",
        "    ####"
      ],
      "metadata": {
        "id": "UHUgcMb65YoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "-Ey2klId6mui",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "05745cf5-9bd9-4c6a-86e4-f55f0a22917e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2da0ffaf5447>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "\n",
        "def test():\n",
        "    torch.manual_seed(123)\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda:0\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-{}-{}-v0\".format(world, stage))\n",
        "    env = JoypadSpace(env, action_type)\n",
        "    env = CustomReward(env)\n",
        "    env = CustomSkipFrame(env)\n",
        "\n",
        "    # local_model = ActorCritic(num_states, num_actions)\n",
        "    local_model = ActorCritic(env.observation_space.shape[0], len(action_type))\n",
        "    local_model.load_state_dict(torch.load(\"./a2c_super_mario_bros_1_1_S.pt\"))\n",
        "\n",
        "    #for name, param in local_model.named_parameters():\n",
        "    #  print(name, param)\n",
        "    #  return\n",
        "\n",
        "    local_model = local_model.to(device)\n",
        "    local_model.eval()\n",
        "\n",
        "    state = torch.from_numpy(env.reset())\n",
        "    done = True\n",
        "\n",
        "    # actions = collections.deque(maxlen=max_actions)\n",
        "\n",
        "    images = []\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "\n",
        "    curr_step = 0\n",
        "    while True:\n",
        "        curr_step += 1\n",
        "        #curr_step += 1\n",
        "        # if done:\n",
        "        #     local_model.load_state_dict(global_model.state_dict())\n",
        "        #with torch.no_grad():\n",
        "        if done:\n",
        "          h_0 = torch.zeros((1, 512), dtype=torch.float)\n",
        "          c_0 = torch.zeros((1, 512), dtype=torch.float)\n",
        "          env.reset()\n",
        "        else:\n",
        "          h_0 = h_0.detach()\n",
        "          c_0 = c_0.detach()\n",
        "\n",
        "        h_0 = h_0.to(device)\n",
        "        c_0 = c_0.to(device)\n",
        "        state = state.to(device)\n",
        "\n",
        "        logits, value, h_0, c_0 = local_model.forward(state, h_0, c_0)\n",
        "        policy = F.softmax(logits, dim=1)\n",
        "        action = torch.argmax(policy).item()\n",
        "        action = int(action)\n",
        "        print(action)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        print(reward)\n",
        "        state = torch.from_numpy(state)\n",
        "        img = env.render(mode='rgb_array')\n",
        "        images.append(img)\n",
        "        #actions.append(action)\n",
        "\n",
        "        #if curr_step > num_global_steps or actions.count(actions[0]) == actions.maxlen:\n",
        "        #    done = True\n",
        "        #if done:\n",
        "        #    curr_step = 0\n",
        "        #    actions.clear()\n",
        "        #    state = env.reset()\n",
        "\n",
        "        # print(info)\n",
        "\n",
        "        if info[\"flag_get\"]:\n",
        "          print(\"World 1 stage 1 completed\")\n",
        "          break\n",
        "\n",
        "        if info[\"time\"] == 0 or curr_step > 1000:\n",
        "          print(\"ERROR\")\n",
        "          break\n",
        "\n",
        "        # state = torch.from_numpy(state)\n",
        "    # print(len(images))\n",
        "    imageio.mimsave(\"./replay.mp4\", [np.array(img) for i, img in enumerate(images)], fps=30)"
      ],
      "metadata": {
        "id": "Xhz3F_WRxWDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y python3-opengl ffmpeg xvfb"
      ],
      "metadata": {
        "id": "sXHdXDHy8yqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "P7MfoXQT8kqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFl-t05k8n1i",
        "outputId": "8feb5ab1-7825-46ae-c3bc-b2708c6dcae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7b32b5abcb20>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "od1gv226yjIL",
        "outputId": "b76dc975-1abb-436b-b098-b5ca12ab6bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "-0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "3\n",
            "0.2\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "-6.5\n",
            "3\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "3\n",
            "0.2\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "-0.1\n",
            "2\n",
            "-0.1\n",
            "2\n",
            "-0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "-6.5\n",
            "2\n",
            "-0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "3\n",
            "0.2\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "-6.5\n",
            "3\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "3\n",
            "0.2\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "-0.1\n",
            "2\n",
            "-0.1\n",
            "2\n",
            "-0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "-6.5\n",
            "2\n",
            "-0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "3\n",
            "0.2\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "-6.5\n",
            "3\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "3\n",
            "0.2\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "-0.1\n",
            "2\n",
            "-0.1\n",
            "2\n",
            "-0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "-6.5\n",
            "2\n",
            "-0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "3\n",
            "0.2\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "2\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "0.3\n",
            "3\n",
            "-6.5\n",
            "3\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.2\n",
            "2\n",
            "0.1\n",
            "3\n",
            "0.2\n",
            "3\n",
            "0.3\n",
            "3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-247-fbd55f77ab7c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-217-a0644307813e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-229-f641ae653918>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-229-f641ae653918>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# print(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nes_py/wrappers/joypad_space.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# take the step and record the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nes_py/nes_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrollers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# pass the action to the emulator as an unsigned byte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;31m# get the reward for this step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def record_video(env, policy, out_directory, fps=30):\n",
        "  \"\"\"\n",
        "  Generate a replay video of the agent\n",
        "  :param env\n",
        "  :param Qtable: Qtable of our agent\n",
        "  :param out_directory\n",
        "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  img = env.render(mode='rgb_array')\n",
        "  images.append(img)\n",
        "  while not done:\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action, _ = policy.act(state)\n",
        "    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ],
      "metadata": {
        "id": "h4viSefjtiiG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}